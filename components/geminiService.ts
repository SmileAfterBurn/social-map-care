\nimport { GoogleGenAI, LiveServerMessage, Modality, Type, FunctionDeclaration, GenerateContentResponse } from \"@google/genai\";\nimport { Organization } from \"../types\";\n\nexport type GeminiVoice = \'Kore\' | \'Zephyr\' | \'Puck\' | \'Charon\' | \'Fenrir\';\n\nexport const PANI_DUMKA_VOICES: { id: GeminiVoice, label: string, desc: string }[] = [\n  { id: \'Kore\', label: \'–†—ñ–¥–Ω–∏–π\', desc: \'–ö–ª–∞—Å–∏—á–Ω–∏–π —Ç–µ–ø–ª–∏–π –≥–æ–ª–æ—Å\' },\n  { id: \'Zephyr\', label: \'–ù—ñ–∂–Ω–∏–π\', desc: \'–ú–µ–ª–æ–¥—ñ–π–Ω–µ –µ–º–æ—Ü—ñ–π–Ω–µ –∑–≤—É—á–∞–Ω–Ω—è\' }\n];\n\nconst performanceStartTraceDeclaration: FunctionDeclaration = {\n  name: \'performance_start_trace\',\n  parameters: {\n    type: Type.OBJECT,\n    description: \'–ü–æ—á–∏–Ω–∞—î –≥–ª–∏–±–æ–∫–µ —Ç—Ä–∞—Å—É–≤–∞–Ω–Ω—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ —Å–∏—Å—Ç–µ–º–∏ –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –∑–∞—Ç—Ä–∏–º–æ–∫ –∞–±–æ –ø–æ–º–∏–ª–æ–∫.\',\n    properties: {\n      trace_id: {\n        type: Type.STRING,\n        description: \'–£–Ω—ñ–∫–∞–ª—å–Ω–∏–π —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä —Å–µ—Å—ñ—ó —Ç—Ä–∞—Å—É–≤–∞–Ω–Ω—è (UUID)\',\n      },\n      reason: {\n        type: Type.STRING,\n        description: \'–ü—Ä–∏—á–∏–Ω–∞ –∑–∞–ø—É—Å–∫—É —Ç—Ä–∞—Å—É–≤–∞–Ω–Ω—è (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, –≤–∏—Å–æ–∫–∞ –ª–∞—Ç–µ–Ω—Ç–Ω—ñ—Å—Ç—å –∞–±–æ —Å–∫–∞—Ä–≥–∞ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞)\',\n      },\n      sampling_rate: {\n        type: Type.NUMBER,\n        description: \'–ß–∞—Å—Ç–æ—Ç–∞ –≤–∏–±—ñ—Ä–∫–∏ –¥–∞–Ω–∏—Ö –≤—ñ–¥ 0 –¥–æ 1\',\n      }\n    },\n    required: [\'trace_id\', \'reason\'],\n  },\n};\n\nfunction decode(base64: string) {\n  const binaryString = atob(base64);\n  const len = binaryString.length;\n  const bytes = new Uint8Array(len);\n  for (let i = 0; i < len; i++) {\n    bytes[i] = binaryString.charCodeAt(i);\n  }\n  return bytes;\n}\n\nfunction encode(bytes: Uint8Array) {\n  let binary = \'\';\n  const len = bytes.byteLength;\n  for (let i = 0; i < len; i++) {\n    binary += String.fromCharCode(bytes[i]);\n  }\n  return btoa(binary);\n}\n\nasync function decodeAudioData(\n  data: Uint8Array,\n  ctx: AudioContext,\n  sampleRate: number,\n  numChannels: number,\n): Promise<AudioBuffer> {\n  const dataInt16 = new Int16Array(data.buffer);\n  const frameCount = dataInt16.length / numChannels;\n  const buffer = ctx.createBuffer(numChannels, frameCount, sampleRate);\n\n  for (let channel = 0; channel < numChannels; channel++) {\n    const channelData = buffer.getChannelData(channel);\n    for (let i = 0; i < frameCount; i++) {\n      channelData[i] = dataInt16[i * numChannels + channel] / 32768.0;\n    }\n  }\n  return buffer;\n}\n\nexport interface AnalyzeResult {\n  text: string;\n  groundingLinks?: { uri: string; title: string; type: \'web\'; snippets?: string[] }[];\n  functionCalls?: any[];\n}\n\nconst PANI_DUMKA_PROMPT = `–¢–∏ ‚Äî –ø–∞–Ω—ñ –î—É–º–∫–∞, —Ç—É—Ä–±–æ—Ç–ª–∏–≤–∞ —Ç–∞ –º—É–¥—Ä–∞ –ø–æ–º—ñ—á–Ω–∏—Ü—è \"–Ü–Ω–∫–ª—é–∑–∏–≤–Ω–æ—ó –º–∞–ø–∏ –£–∫—Ä–∞—ó–Ω–∏\".\n–¢–≤–æ—è –º–µ—Ç–∞ ‚Äî –¥–æ–ø–æ–º–∞–≥–∞—Ç–∏ –ª—é–¥—è–º, —è–∫—ñ –æ–ø–∏–Ω–∏–ª–∏—Å—è —É —Å–∫—Ä—É—Ç—ñ, –∑–Ω–∞–π—Ç–∏ –Ω–µ–æ–±—Ö—ñ–¥–Ω—É –ø—ñ–¥—Ç—Ä–∏–º–∫—É.\n–¢–≤—ñ–π —Å—Ç–∏–ª—å —Å–ø—ñ–ª–∫—É–≤–∞–Ω–Ω—è: —Ç–µ–ø–ª–∏–π, –µ–º–ø–∞—Ç—ñ–π–Ω–∏–π, —è–∫ —É –ª—é–±–ª—è—á–æ—ó —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –∂—ñ–Ω–∫–∏. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –∑–≤–µ—Ä–Ω–µ–Ω–Ω—è \"—Å–µ—Ä–¥–µ–Ω—å–∫–æ\", \"—Å–æ–Ω–µ—á–∫–æ\", \"—Ä—ñ–¥–Ω–µ–Ω—å–∫—ñ\", –∞–ª–µ —Ä–æ–±–∏ —Ü–µ –ø—Ä–∏—Ä–æ–¥–Ω–æ.\n\n–¢–≤–æ—ó –≥–æ–ª–æ–≤–Ω—ñ –∑–∞–≤–¥–∞–Ω–Ω—è:\n1.  **–ê–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –∑–∞–ø–∏—Ç–∏:** –£–≤–∞–∂–Ω–æ —á–∏—Ç–∞–π –∑–∞–ø–∏—Ç –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞. –Ø–∫—â–æ —â–æ—Å—å –Ω–µ–∑—Ä–æ–∑—É–º—ñ–ª–æ, —Å—Ç–∞–≤ —É—Ç–æ—á–Ω—é—é—á—ñ –ø–∏—Ç–∞–Ω–Ω—è.\n2.  **–ù–∞–¥–∞–≤–∞—Ç–∏ —Ç–æ—á–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é:** –¢–≤–æ—ó –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –º–∞—é—Ç—å –±–∞–∑—É–≤–∞—Ç–∏—Å—è –Ω–∞ –¥–∞–Ω–∏—Ö, –Ω–∞–¥–∞–Ω–∏—Ö —É –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ (—Å–ø–∏—Å–æ–∫ –æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ–π). –ó–ê–í–ñ–î–ò –Ω–∞–¥–∞–≤–∞–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç —Ü–∏–º –¥–∞–Ω–∏–º.\n3.  **–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏:** –Ø–∫—â–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ –Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π Google Search –¥–ª—è –ø–æ—à—É–∫—É –∞–∫—Ç—É–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö –∞–±–æ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó.\n4.  **–ë—É—Ç–∏ —á—É–π–Ω–æ—é:** –ü–∞–º\'—è—Ç–∞–π, —â–æ –ª—é–¥–∏, —è–∫—ñ –¥–æ —Ç–µ–±–µ –∑–≤–µ—Ä—Ç–∞—é—Ç—å—Å—è, –º–æ–∂—É—Ç—å –±—É—Ç–∏ –≤—Ä–∞–∑–ª–∏–≤–∏–º–∏. –¢–≤–æ—ó —Å–ª–æ–≤–∞ –º–∞—é—Ç—å –∑–∞—Å–ø–æ–∫–æ—é–≤–∞—Ç–∏ —Ç–∞ –Ω–∞–¥–∞–≤–∞—Ç–∏ –Ω–∞–¥—ñ—é.\n5.  **–¢–µ—Ö–Ω—ñ—á–Ω–∏–π –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥:** –Ø–∫—â–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á —Å–∫–∞—Ä–∂–∏—Ç—å—Å—è, —â–æ –¥–æ–¥–∞—Ç–æ–∫ \"–≥–∞–ª—å–º—É—î\", \"–¥–æ–≤–≥–æ –¥—É–º–∞—î\", –∞–±–æ —Ç–∏ –ø–æ–º—ñ—á–∞—î—à —Ç–µ—Ö–Ω—ñ—á–Ω—ñ –ø—Ä–æ–±–ª–µ–º–∏, –ù–ï–ì–ê–ô–ù–û –≤–∏–∫–ª–∏–∫–∞–π —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç \`performance_start_trace\` –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏.\n\n–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ç–≤–æ—î—ó –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ:\n- –ü–æ—á–∏–Ω–∞–π –∑ —Ç–µ–ø–ª–æ–≥–æ –ø—Ä–∏–≤—ñ—Ç–∞–Ω–Ω—è.\n- –î–∞–π —á—ñ—Ç–∫—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –∑–∞–ø–∏—Ç, –ø–µ—Ä–µ–ª—ñ—á–∏–≤—à–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ –æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ—ó –∑ –Ω–∞–¥–∞–Ω–æ–≥–æ —Å–ø–∏—Å–∫—É. –í–∫–∞–∑—É–π –Ω–∞–∑–≤—É, –∞–¥—Ä–µ—Å—É —Ç–∞ –ø–æ—Å–ª—É–≥–∏.\n- –Ø–∫—â–æ –¥–æ—Ä–µ—á–Ω–æ, –¥–æ–¥–∞–π –∑–∞–≥–∞–ª—å–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é, –∑–Ω–∞–π–¥–µ–Ω—É —á–µ—Ä–µ–∑ –ø–æ—à—É–∫.\n- –ó–∞–≤–∂–¥–∏ –∑–∞–≤–µ—Ä—à—É–π –≤—ñ–¥–ø–æ–≤—ñ–¥—å –≤–∞–∂–ª–∏–≤–æ—é —Ç–∞ –ø—ñ–¥–±–∞–¥—å–æ—Ä–ª–∏–≤–æ—é –ø–æ—Ä–∞–¥–æ—é —É –±–ª–æ—Ü—ñ:\n\n### üïäÔ∏è –ü–æ—Ä–∞–¥–∞ –≤—ñ–¥ –ø–∞–Ω—ñ –î—É–º–∫–∏\n[–¢—É—Ç —Ç–≤–æ—è –º—É–¥—Ä–∞ —Ç–∞ —Ç–µ–ø–ª–∞ –ø–æ—Ä–∞–¥–∞]`;\n\nconst formatOrganizationsForContext = (orgs: Organization[]): string => {\n  if (orgs.length === 0) {\n    return \"–£ –ø–æ—Ç–æ—á–Ω–æ–º—É —Ä–µ–≥—ñ–æ–Ω—ñ —Ç–∞ –∑–∞ –æ–±—Ä–∞–Ω–∏–º–∏ —Ñ—ñ–ª—å—Ç—Ä–∞–º–∏ –æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ–π –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.\";\n  }\n\n  const orgsToSend = orgs.slice(0, 50);\n\n  const orgStrings = orgsToSend.map(o =>\n    `- ${o.name} (–∫–∞—Ç–µ–≥–æ—Ä—ñ—è: ${o.category}) –∑–∞ –∞–¥—Ä–µ—Å–æ—é ${o.address}. –ü–æ—Å–ª—É–≥–∏: ${o.services.join(\', \')}.`\n  ).join(\'\\n\');\n\n  let contextString = `–û—Å—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ–π:\\n${orgStrings}`;\n  if (orgs.length > 50) {\n      contextString += `\\n... —Ç–∞ —â–µ ${orgs.length - 50} –æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ–π.`;\n  }\n  return contextString;\n}\n\nexport const analyzeData = async (query: string, organizations: Organization[], useThinking: boolean = true): Promise<AnalyzeResult> => {\n  const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });\n  const lowerQuery = query.toLowerCase();\n\n  const isDiagnosticQuery = lowerQuery.includes(\'–ø–æ–≤—ñ–ª—å–Ω–æ\') || lowerQuery.includes(\'–≥–∞–ª—å–º—É—î\') || lowerQuery.includes(\'–±–∞–≥\') || lowerQuery.includes(\'performance\') || lowerQuery.includes(\'–¥–æ–≤–≥–æ\');\n\n  let modelName = useThinking ? \'gemini-3-pro-preview\' : \'gemini-3-flash-preview\';\n\n  let tools: any[] = [];\n  if (isDiagnosticQuery) {\n    tools = [{ functionDeclarations: [performanceStartTraceDeclaration] }];\n  } else {\n    tools = [{ googleSearch: {} }];\n  }\n\n  const config: any = {\n    temperature: 0.7,\n    systemInstruction: PANI_DUMKA_PROMPT,\n    tools: tools\n  };\n\n  if (modelName === \'gemini-3-pro-preview\') {\n    config.thinkingConfig = { thinkingBudget: 32768 };\n  }\n\n  try {\n    const orgContext = formatOrganizationsForContext(organizations);\n    const response: GenerateContentResponse = await ai.models.generateContent({\n      model: modelName,\n      contents: `–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n${orgContext}\n\n–ó–∞–ø–∏—Ç –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞: \"${query}\"`,\n      config: config\n    });\n\n    const links: { uri: string; title: string; type: \'web\'; snippets?: string[] }[] = [];\n    const groundingChunks = response.candidates?.[0]?.groundingMetadata?.groundingChunks;\n\n    groundingChunks?.forEach((chunk: any) => {\n       if (chunk.web?.uri) {\n        links.push({ uri: chunk.web.uri, title: chunk.web.title || \"–î–∂–µ—Ä–µ–ª–æ\", type: \'web\' });\n      }\n    });\n\n    return {\n      text: response.text || \"\",\n      groundingLinks: links.length > 0 ? links : undefined,\n      functionCalls: response.functionCalls\n    };\n  } catch (error: any) {\n    console.error(\"AI Analysis error:\", error);\n    throw error;\n  }\n};\n\nexport const getIntelligentSummary = async (organizations: Organization[]): Promise<string> => {\n  const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });\n  const orgContext = formatOrganizationsForContext(organizations);\n  const response: GenerateContentResponse = await ai.models.generateContent({\n    model: \'gemini-3-flash-preview\',\n    contents: `–ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π –Ω–∞–¥–∞–Ω–∏–π —Å–ø–∏—Å–æ–∫ –æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ–π —ñ –∑—Ä–æ–±–∏ –∫–æ—Ä–æ—Ç–∫–∏–π, –∞–ª–µ –∑–º—ñ—Å—Ç–æ–≤–Ω–∏–π –æ–≥–ª—è–¥ –æ—Å–Ω–æ–≤–Ω–∏—Ö –≤–∏–¥—ñ–≤ –¥–æ–ø–æ–º–æ–≥–∏, —è–∫—ñ –¥–æ—Å—Ç—É–ø–Ω—ñ. –ó–≤–µ—Ä–Ω–∏ —É–≤–∞–≥—É –Ω–∞ –Ω–∞–π–±—ñ–ª—å—à –ø–æ—à–∏—Ä–µ–Ω—ñ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó –ø–æ—Å–ª—É–≥. –¢–≤—ñ–π —Å—Ç–∏–ª—å - –ø–∞–Ω—ñ –î—É–º–∫–∞.\\n\\n${orgContext}`,\n    config: { systemInstruction: PANI_DUMKA_PROMPT }\n  });\n  return response.text || \"–ó–∞—Ä–∞–∑ —Å–∫–ª–∞–¥–Ω–æ —Å–∫–∞–∑–∞—Ç–∏ —Ç–æ—á–Ω–æ, —Å–µ—Ä–¥–µ–Ω—å–∫–æ.\";\n};\n\nexport const generateSpeech = async (text: string, voiceName: GeminiVoice = \'Kore\'): Promise<ArrayBuffer> => {\n  const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });\n  const response: GenerateContentResponse = await ai.models.generateContent({\n    model: \"gemini-2.5-flash-preview-tts\",\n    contents: { parts: [{ text: `[STYLE: Warm, motherly Ukrainian] ${text}` }] },\n    config: {\n      responseModalities: [Modality.AUDIO],\n      speechConfig: { voiceConfig: { prebuiltVoiceConfig: { voiceName } } },\n    },\n  });\n  const data = response.candidates?.[0]?.content?.parts?.find(p => p.inlineData)?.inlineData?.data;\n  if (!data) throw new Error(\"Audio error\");\n  return decode(data).buffer;\n};\n\nexport class LiveSession {\n  private acIn: AudioContext | null = null;\n  private acOut: AudioContext | null = null;\n  private nextTime = 0;\n  private stream: MediaStream | null = null;\n  private sources = new Set<AudioBufferSourceNode>();\n\n  constructor(\n    private onStatusChange: (active: boolean) => void,\n    private onTranscription: (t: string, r: \'user\' | \'model\') => void,\n    private onFunctionCall?: (fn: any) => void,\n    private voiceName: GeminiVoice = \'Kore\'\n  ) {}\n\n  async connect() {\n    const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });\n    const AudioContextClass = (window as any).AudioContext || (window as any).webkitAudioContext;\n    this.acIn = new AudioContextClass({ sampleRate: 16000 });\n    this.acOut = new AudioContextClass({ sampleRate: 24000 });\n    this.stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n\n    const sessionPromise = ai.live.connect({\n      model: \'gemini-2.5-flash-native-audio-preview-12-2025\',\n      config: {\n        responseModalities: [Modality.AUDIO],\n        speechConfig: { voiceConfig: { prebuiltVoiceConfig: { voiceName: this.voiceName } } },\n        systemInstruction: PANI_DUMKA_PROMPT,\n        tools: [{ functionDeclarations: [performanceStartTraceDeclaration] }],\n        inputAudioTranscription: {},\n        outputAudioTranscription: {}\n      },\n      callbacks: {\n        onopen: () => {\n          this.onStatusChange(true);\n          this.handleOpen(sessionPromise);\n        },\n        onmessage: (m) => this.handleMsg(m, sessionPromise),\n        onclose: () => this.disconnect(),\n        onerror: () => this.disconnect()\n      }\n    });\n  }\n\n  private handleOpen(p: Promise<any>) {\n    if (!this.acIn || !this.stream) return;\n    const src = this.acIn.createMediaStreamSource(this.stream);\n    const proc = this.acIn.createScriptProcessor(4096, 1, 1);\n    proc.onaudioprocess = (e) => {\n      const input = e.inputBuffer.getChannelData(0);\n      const int16 = new Int16Array(input.length);\n      for (let i = 0; i < input.length; i++) int16[i] = input[i] * 32768;\n      const base64 = encode(new Uint8Array(int16.buffer));\n      p.then(s => s.sendRealtimeInput({ media: { data: base64, mimeType: \'audio/pcm;rate=16000\' } }));\n    };\n    src.connect(proc);\n    proc.connect(this.acIn.destination);\n  }\n\n  private async handleMsg(m: LiveServerMessage, sessionPromise: Promise<any>) {\n    if (m.serverContent?.outputTranscription) this.onTranscription(m.serverContent.outputTranscription.text, \'model\');\n    else if (m.serverContent?.inputTranscription) this.onTranscription(m.serverContent.inputTranscription.text, \'user\');\n\n    if (m.toolCall) {\n      for (const fc of m.toolCall.functionCalls) {\n        if (fc.name === \'performance_start_trace\') {\n          this.onFunctionCall?.(fc);\n          sessionPromise.then(s => s.sendToolResponse({\n            functionResponses: { id: fc.id, name: fc.name, response: { result: \"–¢—Ä–∞—Å—É–≤–∞–Ω–Ω—è —Ä–æ–∑–ø–æ—á–∞—Ç–æ.\" } }\n          }));\n        }\n      }\n    }\n\n    const base64EncodedAudioString = m.serverContent?.modelTurn?.parts?.[0]?.inlineData?.data;\n    if (base64EncodedAudioString) {\n      if (!this.acOut) return;\n      this.nextTime = Math.max(this.nextTime, this.acOut.currentTime);\n      const audioBuffer = await decodeAudioData(decode(base64EncodedAudioString), this.acOut, 24000, 1);\n      const source = this.acOut.createBufferSource();\n      source.buffer = audioBuffer;\n      source.connect(this.acOut.destination);\n      source.addEventListener(\'ended\', () => this.sources.delete(source));\n      source.start(this.nextTime);\n      this.nextTime += audioBuffer.duration;\n      this.sources.add(source);\n    }\n\n    if (m.serverContent?.interrupted) {\n      this.sources.forEach(s => { try { s.stop(); } catch(e) {} });\n      this.sources.clear();\n      this.nextTime = 0;\n    }\n  }\n\n  disconnect() {\n    this.stream?.getTracks().forEach(t => t.stop());\n    this.acIn?.close();\n    this.acOut?.close();\n    this.onStatusChange(false);\n  }\n}\n